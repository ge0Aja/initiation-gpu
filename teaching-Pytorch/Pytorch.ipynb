{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will use Pytorch to do solve 101 Deep learning problems.\n",
    "\n",
    "These excercies will focus on the use of Pytorch to define, train and use neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network achitecture\n",
    "\n",
    "The first step is to define some neural network architecture.\n",
    "\n",
    "In Pytorch we use the `Module` class.\n",
    "Basically a `Module` is a block that can do *forward* and *backward* propagation.\n",
    "Every part of a neural network is a `Module`.\n",
    "\n",
    "You can define a custom `Module` subclass by combining already existing modules and functions.\n",
    "To do so you have to initialize the sublayer if needed and define the `forward()` method.\n",
    "\n",
    "Example :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectifiedSigmoidLayer(nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(D_in, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Use GPU for your computations\n",
    "\n",
    "Using GPUs for Deep Learning allows to accelerate your computations: As most of the computations are matrix-based operations, they are heavily parallelizable. In PyTorch, using Nvidia GPUs comes in 3 steps:\n",
    "\n",
    "1. Send your model to the GPU\n",
    "2. Send your data to the GPU\n",
    "3. Perform your computations as usual\n",
    "4. (Optional) Retrieve the results to the CPU.\n",
    "\n",
    "Performing these operations is actually easy:\n",
    "```python\n",
    "device = \"cuda:0\"  \n",
    "# for the first GPU, and 'cpu' for the CPU, making the code very architechture independent.\n",
    "\n",
    "#1. \n",
    "model = nn.Linear(50,1)\n",
    "model = model.to(device)\n",
    "\n",
    "#2.\n",
    "data = torch.randn(500,50)\n",
    "data = data.to(device)\n",
    "\n",
    "#3. Forward pass here.\n",
    "output = model(data)\n",
    "\n",
    "#4.\n",
    "output_cpu = output.cpu().numpy() # + conversion to numpy array.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Play with number of layer and neurons\n",
    "\n",
    "A neural network is also a `Module` since we can do forward and backward propagation on it.\n",
    "\n",
    "Complex neural network structures are often built with smaller repetitive blocks. \n",
    "Let's practice building those blocks.\n",
    "\n",
    "For all these question the input and output dimension are user-defined. The non-linearities should be ReLU.\n",
    "\n",
    "1. Define a 3 layers fully connected neural network with 40 neurons in each layer. \n",
    "2. Same as previous question but with a *n*-layers neural network (*n* is given by user). Astuce `nn.ModuleList` or `nn.ModuleDict`\n",
    "3. Define a *n*-layer fully connected network in which the number of neuron start at *k* and double at each layer.\n",
    "4. Define a *n*-layer fully connected network in which the number of neuron start at *k* and is divided by 2 at each layer.\n",
    "5. Define a fully connected network for which the number of neurons for each layer is given as a list. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo1Q1(nn.Module):\n",
    "    \"\"\"Define a 3 layers fu≈ìlly connected neural network with 40 neurons in each layer.\"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Exo1Q1(18, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo1Q2(nn.Module):\n",
    "    \"\"\"Same as previous question but with a n-layers neural network (n is given by user)\"\"\"\n",
    "    def __init__(self, D_in, D_out, n_layer):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Exo1Q2(18, 2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo1Q3(nn.Module):\n",
    "    \"\"\"Define a n-layer fully connected network in which the number of neuron start at k and double at each layer.\"\"\"\n",
    "    def __init__(self, D_in, D_out, k, n_layer):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Exo1Q3(18, 3, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo1Q4(nn.Module):\n",
    "    \"\"\"Define a n-layer fully connected network in which the number of neuron start at k\n",
    "    and is divided by 2 at each layer.\"\"\"\n",
    "    def __init__(self, D_in, D_out, k, n_layer):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Exo1Q4(38, 3, 80, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo1Q5(nn.Module):\n",
    "    \"\"\"Define a fully connected network for which the number of neurons for each layer is given as a list\"\"\"\n",
    "    def __init__(self, D_in, D_out, n_neurons):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Exo1Q5(30, 4, [5, 7, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutionnal layers\n",
    "\n",
    "When the input features have some spatial structure as in images (nearby pixels) or in sound or text (nearby letter/word) the use of convolution layer is relevant to extract local structure.\n",
    "\n",
    "With convolution layer the output shape depend on the input shape. See https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d for documentation.\n",
    "\n",
    "Moreover convolution layer are almost always followed by some dimension reduction with max pooling or average pooling.\n",
    "\n",
    "We can use convolution in the same way as fully connected. See example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ConvNet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : draw me a sheep\n",
    "\n",
    "1. Draw a diagram of the previously defined ConvNet. The diagram should show each operation (conv layer, dense layer, max pooling, non-linearity, dropout, etc) applied to the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : More complex structure\n",
    "\n",
    "Most neural networks are sequential neural networks but it is just as easy to build more complex structure.\n",
    "\n",
    "Using the forward() method define the following structures :\n",
    "\n",
    "1. A simple addition ![First structure](exo3Q1.png \"Simple add\")\n",
    "2. Shorcuts can help training (see *deep residual learning*) ![Second structure](exo3Q2.png \"Shortcuts\")\n",
    "3. You want to go crazy ? Reimplement some of the [Inception modules](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo3Q1(nn.Module):\n",
    "    \"\"\"Define a fully connected network for which the number of neurons for each layer is given as a list\"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo3Q2a(nn.Module):\n",
    "    \"\"\"Define a fully connected network for which the number of neurons for each layer is given as a list\"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exo3Q2SubBlock(nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "\n",
    "        \n",
    "class Exo3Q2b(nn.Module):\n",
    "    \"\"\"Define a fully connected network for which the number of neurons for each layer is given as a list\"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "The \"hello world\" of Machine Learning.\n",
    "\n",
    "The MNIST dataset contains 70k 28x28 grayscale images of handwritten digits \"0123456789\".\n",
    "The name of the game is to automatically recognize the digit from the image.\n",
    "\n",
    "Once the neural network architecture is defined one should focus on training it to some task.\n",
    "Here it will be classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mydata.mnist import load_mnist\n",
    "\n",
    "X, y = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "for ax in np.ravel(axes):\n",
    "    i = np.random.randint(0, n_sample)\n",
    "    img = X[i].reshape(28,28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(\"label = {}\".format(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Here is a bunch of function that will ease data handling.\n",
    "\n",
    "\n",
    "First, we will need to feed the neural network with small batch of the training data.\n",
    "Mini-batch gradient descent is the best way to optimize models and has been unchallenged for quite some time now.\n",
    "\n",
    "We define here some functions to shuffle and split some arrays and yield them as small bunch.\n",
    "\n",
    "*epoch_shuffle()* that will produce minibatch of the given arrays and reshuffle the data when it finishes an epoch. This means *epoch_shuffle()* will never stop sending minibatches !\n",
    "\n",
    "This :\n",
    "```python\n",
    "for x in epoch_shuffle(data):\n",
    "    do stuff\n",
    "```\n",
    "IS INFINITE LOOP\n",
    "\n",
    "\n",
    "\n",
    "*one_epoch()*  will not shuffle the data and will stop once all the samples has been returned one time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_arrays_have_same_shape(*arrays):\n",
    "    length = arrays[0].shape[0]\n",
    "    # Assert that every array have the same 1st dimension length:\n",
    "    for i, arr in enumerate(arrays):\n",
    "        assert arr.shape[0] == length, \"Every array should have the same shape: \" \\\n",
    "            \" array {} length = {}  but expected length = {} \".format(i + 1, arr.shape[0], length)\n",
    "\n",
    "\n",
    "def epoch_shuffle(*args, batch_size=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Return a generator taking 'batch_size' random samples from X and y.\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        raise ValueError('batch_size should not be None !')\n",
    "    if len(args) == 0:\n",
    "        raise ValueError('minibatching must take at least one array')\n",
    "    assert_arrays_have_same_shape(*args)\n",
    "\n",
    "    size = args[0].shape[0]\n",
    "    assert size > batch_size, 'batch_size should be smaller than the number of samples in the given arrays'\n",
    "\n",
    "    while(True):\n",
    "        if shuffle:\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, size - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield tuple(arr[excerpt] for arr in args)\n",
    "\n",
    "\n",
    "def one_epoch(*arrays, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        raise ValueError('batch_size should not be None !')\n",
    "    if len(arrays) == 0:\n",
    "        raise ValueError('minibatching must take at least one array')\n",
    "    assert_arrays_have_same_shape(*arrays)\n",
    "    \n",
    "    size = arrays[0].shape[0]\n",
    "    assert size > batch_size, 'batch_size should be smaller than the number of samples in the given arrays'\n",
    "    \n",
    "    for i in range(0, size, batch_size):\n",
    "        excerpt = slice(i, i + batch_size)\n",
    "        yield tuple(arr[excerpt] for arr in arrays) if len(arrays) > 1 else arrays[0][excerpt]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the training\n",
    "\n",
    "Then the training algorithm is quite simple :\n",
    "\n",
    "1. turn data into minibatch\n",
    "2. for n iterations :\n",
    "    1. turn minibatch array into variables\n",
    "    2. clean the optimizer (it accumulates the gradient by default)\n",
    "    3. do forward propagation through the network\n",
    "    4. do forward propagation through the loss function\n",
    "    5. do backward propagation to compute the gradients\n",
    "    6. use the optimizer to do a gradient descent step (update the trainable parameters of the network)\n",
    "    7. monitor some stuff\n",
    "3. return some training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_generator, n_iter=5000, cuda=True, log_interval=100):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    # Turn data into minibatch\n",
    "    for batch_idx, (X_batch, y_batch) in zip(range(n_iter), data_generator):\n",
    "        # turn minibatch into variables\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        y_batch = torch.from_numpy(y_batch)\n",
    "        if cuda :\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # clean optimizer (it accumulates the gradient by default) \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # do forward propagation through the network\n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        # do forward propagation through the loss function\n",
    "        loss = F.nll_loss(y_pred, y_batch)\n",
    "        \n",
    "        # do backward propagation to compute the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # use the optimizer to do a gradient descent step (update the trainable parameters of the network)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # monitor some stuff\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train iter: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx, n_iter, 100. * batch_idx / n_iter, loss.item()))\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data, the neural network, the optimizer, the minibatch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image to expected [n_sample, n_channel, width, height]\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# cast uint8 to long\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Get the network\n",
    "net = ConvNet()\n",
    "\n",
    "# Send the network to gpu\n",
    "net = net.cuda()\n",
    "\n",
    "# The optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "#optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# minibatch\n",
    "data_generator = epoch_shuffle(X_train, y_train, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train(net, optimizer, data_generator, n_iter=5000, cuda=True, log_interval=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"# iter\")\n",
    "plt.ylabel(\"loss (nll)\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses + train(net, optimizer, data_generator, n_iter=5000, cuda=True, log_interval=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"# iter\")\n",
    "plt.ylabel(\"loss (nll)\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, batch_size=512, cuda=True):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # speed up computation but makes backward propagation impossible\n",
    "        for X_batch in one_epoch(X, batch_size=batch_size):\n",
    "            X_batch = torch.from_numpy(X_batch)\n",
    "            if cuda:\n",
    "                X_batch = X_batch.cuda()\n",
    "            y_log_proba = model(X_batch)\n",
    "            y_pred = y_log_proba.max(1, keepdim=True)[1]\n",
    "            predictions.extend(y_pred.cpu().data.numpy())\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(net, X_test)\n",
    "print('accuracy : ', np.mean(y_pred.ravel() == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is one iteration of the search to solve a problem with deep learning.\n",
    "- Define some netowrk structure\n",
    "- Setup training/testing data\n",
    "- Run the gradient descent algorithm\n",
    "- Analyse monitored information and the final performances\n",
    "\n",
    "This loop is parametrized by many parameter:\n",
    "- Optimizer choice (SGD, Nesterov, Adam, AdaGrad, etc)\n",
    "- learning rate, learning rate momentum, cycle length, etc\n",
    "- Neural network architecture\n",
    "- minibatch size\n",
    "- dropout probability\n",
    "- etc\n",
    "\n",
    "They are called **hyper-parameters** as opposed to **parameters** that usually refer to the parameters that are trained with gradient descent.\n",
    "\n",
    "The next part will focus on the implementation of some hyper-parameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter search\n",
    "\n",
    "As stated in the previous section hyper-parameter search can be boiled down to a big loop gathering the model definition, training and evaluation.\n",
    "\n",
    "In order to enable reproducible experiment it is necessary to have one fixed seed for every generator.\n",
    "Some randomness may remain because float operation are not commutative or from multi-threaded operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 15081715  # your birthdate, 42, your phone number\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The critical part of hyper-parameter optimization is the evaluation of the performances of a given model.\n",
    "\n",
    "Here the objective is to get the **highest accuracy** on the test set.\n",
    "\n",
    "A good practice is to measure some average and variance of the performance according to data split and random initialization. This is achieved with cross-validation ie we execute the evaluation on different split of the data into a training and validation set and different random initialization.\n",
    "\n",
    "Since we will use the validation data to choose some parameters of the model it is possible (very likely) that we overfit on the validation set. This is why it is absolutely necessary to **keep some data in the vault and never touch it until the very end !**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = X[:-10000]\n",
    "y_dev = y[:-10000]\n",
    "X_test = X[-10000:]  # NEVER touch it again\n",
    "y_test = y[-10000:]  # NEVER touch it again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of cross-validation split.\n",
    "No need to reinvent the wheel. Use scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "dummy_X_dev = np.arange(100)\n",
    "dummy_y_dev = np.repeat(np.arange(10), 10)\n",
    "\n",
    "cv_splitter = ShuffleSplit(n_splits=5, test_size=0.1, random_state=RANDOM_SEED)\n",
    "cv_generator = cv_splitter.split(dummy_X_dev, dummy_y_dev)\n",
    "\n",
    "for train_idx, valid_idx in cv_generator:\n",
    "    X_train = dummy_X_dev[train_idx]\n",
    "    X_valid = dummy_X_dev[valid_idx]\n",
    "    y_train = dummy_y_dev[train_idx]\n",
    "    y_valid = dummy_y_dev[valid_idx]\n",
    "    # Do stuff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Pimp my cross-validation\n",
    "\n",
    "1. Make use of *train()* and *predict()* and the previous example to write *evaluation()* function that computes *mean* and *std* of the validation accuracy.\n",
    "2. Improve the *train()* function to monitor validation loss after some *log_interval* (don't forget to switch the model between evaluation and training mode).\n",
    "3. Improve the *evaluate()* function to save the trained models (use [state_dict methods](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "4. Improve the *train()* function to regularly save training model checkpoint. This is usefull to enable recovery from a crash on the cluster or on the cloud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_v1():\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it\n",
    "\n",
    "evaluation_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v2():\n",
    "    pass # TODO\n",
    "\n",
    "def evaluation_v2():\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it\n",
    "\n",
    "evaluation_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_v3():\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it\n",
    "\n",
    "evaluation_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v4():\n",
    "    pass # TODO\n",
    "\n",
    "def evaluation_v4():\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it\n",
    "\n",
    "evaluation_v4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter sampler\n",
    "\n",
    "- Grid search vs random search vs fancy black box optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN architechtures\n",
    "\n",
    "GAN architechtures are quite pelicular, where the metric of the NN is also computed with a NN. \n",
    "\n",
    "\n",
    "![gan](gan.png)\n",
    "\n",
    "This structure relies on a double-optimization game, where the discriminator learns to distinguish the true images from the false images, and the generator tries to fool the discriminator.\n",
    "\n",
    "This game is implemented using two optimizers and the `.detach()` function, that blocks the retropropagation of the gradient. \n",
    "\n",
    "\n",
    "## Exercice 5: Implement a GAN on MNIST, using 1 convolutional layers, and batchnorms.\n",
    "\n",
    "The general structure is the following:\n",
    "\n",
    "1. Pass noise into generator to generate a batch of images \n",
    "2. Pass the generated batch and a true batch through discriminator\n",
    "3. Apply a BCE on the true labels to train the discriminator (while applying `.detach()` on the generated batch)\n",
    "4. Backpropagate the loss and apply the discriminator optimizer\n",
    "5. Apply a BCE on the generated batch using fake labels (saying that they're real)\n",
    "6. Backpropagate and apply the generator optimizer (don't forget to `.zero_grad()`the optimizers!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comments\n",
    "\n",
    "\n",
    "- **Use Object programming** to write clean code\n",
    "- Heavily **use code versionning** because it is very easy to break your model\n",
    "    - \"WTF ?! There is NaN values everywhere !\"\n",
    "    - \"WTF ?! I changed the seed and now I lost 5% of accuracy !\"\n",
    "    - \"WTF ?! I changed 1 neuron number and now there is crazy shape errors everywhere !\"\n",
    "    - \"WTF ?! I changed the batchsize to make better use of the new GPU and now training don't converge !\"\n",
    "- Pytorch/Tensorflow/Keras are not very stable : **use docker** to help yourself and other people to use the code \n",
    "- Pytorch contains many other useful features :\n",
    "    - [forward and backward hooks](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks) to help monitoring of make crazy algorithm\n",
    "    - multi threaded loader for big dataset\n",
    "    - torchvision : almost everything you may want when handling images\n",
    "    - \n",
    "- Do not use Jupyter-notebook for more than prototyping. Once you code is stable/clean enough write a module or a script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "823px",
    "left": "0px",
    "right": "1561px",
    "top": "111px",
    "width": "359px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
